Using ssh batch size of 32. Attempting to SSH into 1 nodes with a total of 32 workers.
SSH: Attempting to connect to worker 0...
SSH: Attempting to connect to worker 1...
SSH: Attempting to connect to worker 2...
SSH: Attempting to connect to worker 3...
SSH: Attempting to connect to worker 4...
SSH: Attempting to connect to worker 5...
SSH: Attempting to connect to worker 6...
SSH: Attempting to connect to worker 7...
SSH: Attempting to connect to worker 8...
SSH: Attempting to connect to worker 9...
SSH: Attempting to connect to worker 10...
SSH: Attempting to connect to worker 11...
SSH: Attempting to connect to worker 12...
SSH: Attempting to connect to worker 13...
SSH: Attempting to connect to worker 14...
SSH: Attempting to connect to worker 15...
SSH: Attempting to connect to worker 16...
SSH: Attempting to connect to worker 17...
SSH: Attempting to connect to worker 18...
SSH: Attempting to connect to worker 19...
SSH: Attempting to connect to worker 20...
SSH: Attempting to connect to worker 21...
SSH: Attempting to connect to worker 22...
SSH: Attempting to connect to worker 23...
SSH: Attempting to connect to worker 24...
SSH: Attempting to connect to worker 25...
SSH: Attempting to connect to worker 26...
SSH: Attempting to connect to worker 27...
SSH: Attempting to connect to worker 28...
SSH: Attempting to connect to worker 29...
SSH: Attempting to connect to worker 30...
SSH: Attempting to connect to worker 31...
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
I0310 23:32:03.320300 140187042658304 llama.py:574] Using fused attention for tpu
I0310 23:32:03.383846 140313205221376 llama.py:574] Using fused attention for tpu
init block_size 32768
init block_size 32768
init block_size 32768
I0310 23:32:03.474456 139682293938176 llama.py:574] Using fused attention for tpu
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
I0310 23:32:04.456918 139733703407616 llama.py:574] Using fused attention for tpu
I0310 23:32:04.531640 139693318477824 llama.py:574] Using fused attention for tpu
init block_size 32768
I0310 23:32:04.620291 140187042658304 llama.py:574] Using fused attention for tpu
I0310 23:32:04.654750 140346975668224 llama.py:574] Using fused attention for tpu
I0310 23:32:04.679448 140313205221376 llama.py:574] Using fused attention for tpu
I0310 23:32:04.778363 139682293938176 llama.py:574] Using fused attention for tpu
init block_size 32768
init block_size 32768
init block_size 32768
init block_size 32768
I0310 23:32:05.164296 140301649876992 llama.py:574] Using fused attention for tpu
init block_size 32768
init block_size 32768
I0310 23:32:05.235991 140396214429696 llama.py:574] Using fused attention for tpu
I0310 23:32:05.261785 140312624297984 llama.py:574] Using fused attention for tpu
I0310 23:32:05.272100 140296425859072 llama.py:574] Using fused attention for tpu
I0310 23:32:05.305685 140266063370240 llama.py:574] Using fused attention for tpu
I0310 23:32:05.323359 139813277104128 llama.py:574] Using fused attention for tpu
I0310 23:32:05.351840 139833263335424 llama.py:574] Using fused attention for tpu
I0310 23:32:05.379061 139772488329216 llama.py:574] Using fused attention for tpu
init block_size 32768
I0310 23:32:05.476316 140063402936320 llama.py:574] Using fused attention for tpu
I0310 23:32:05.533919 140120372094976 llama.py:574] Using fused attention for tpu
I0310 23:32:05.587932 139789605083136 llama.py:574] Using fused attention for tpu
I0310 23:32:05.605713 139637727176704 llama.py:574] Using fused attention for tpu
I0310 23:32:05.613367 139761310316544 llama.py:574] Using fused attention for tpu
I0310 23:32:05.761843 139733703407616 llama.py:574] Using fused attention for tpu
I0310 23:32:05.820708 139693318477824 llama.py:574] Using fused attention for tpu
I0310 23:32:05.952525 139986880280576 llama.py:574] Using fused attention for tpu
I0310 23:32:05.953146 140346975668224 llama.py:574] Using fused attention for tpu
init block_size 32768
init block_size 32768
I0310 23:32:06.226478 139968301377536 llama.py:574] Using fused attention for tpu
I0310 23:32:06.339602 140044392253440 llama.py:574] Using fused attention for tpu
I0310 23:32:06.464598 140301649876992 llama.py:574] Using fused attention for tpu
I0310 23:32:06.531284 140396214429696 llama.py:574] Using fused attention for tpu
I0310 23:32:06.544282 140296425859072 llama.py:574] Using fused attention for tpu
I0310 23:32:06.567175 140312624297984 llama.py:574] Using fused attention for tpu
I0310 23:32:06.604061 140266063370240 llama.py:574] Using fused attention for tpu
I0310 23:32:06.630417 139813277104128 llama.py:574] Using fused attention for tpu
I0310 23:32:06.674450 139833263335424 llama.py:574] Using fused attention for tpu
I0310 23:32:06.679117 139772488329216 llama.py:574] Using fused attention for tpu
I0310 23:32:06.771896 140063402936320 llama.py:574] Using fused attention for tpu
I0310 23:32:06.838101 140120372094976 llama.py:574] Using fused attention for tpu
I0310 23:32:06.887260 139789605083136 llama.py:574] Using fused attention for tpu
I0310 23:32:06.914927 139637727176704 llama.py:574] Using fused attention for tpu
I0310 23:32:06.916497 139761310316544 llama.py:574] Using fused attention for tpu
I0310 23:32:07.108165 140238231980032 llama.py:574] Using fused attention for tpu
I0310 23:32:07.163264 140249725908992 llama.py:574] Using fused attention for tpu
I0310 23:32:07.254175 139986880280576 llama.py:574] Using fused attention for tpu
I0310 23:32:07.547571 140403523438592 llama.py:574] Using fused attention for tpu
I0310 23:32:07.638936 140044392253440 llama.py:574] Using fused attention for tpu
I0310 23:32:07.792064 139968301377536 llama.py:574] Using fused attention for tpu
I0310 23:32:08.561522 140321874843648 llama.py:574] Using fused attention for tpu
I0310 23:32:08.581822 140109931255808 llama.py:574] Using fused attention for tpu
I0310 23:32:08.721781 139829679552512 llama.py:574] Using fused attention for tpu
I0310 23:32:08.866309 140590184425472 llama.py:574] Using fused attention for tpu
I0310 23:32:09.093250 140028743354368 llama.py:574] Using fused attention for tpu
I0310 23:32:09.723397 139908633098240 llama.py:574] Using fused attention for tpu
I0310 23:32:09.812421 139684034746368 llama.py:574] Using fused attention for tpu
I0310 23:32:09.882901 140321874843648 llama.py:574] Using fused attention for tpu
I0310 23:32:09.903116 140109931255808 llama.py:574] Using fused attention for tpu
I0310 23:32:10.572476 139829679552512 llama.py:574] Using fused attention for tpu
I0310 23:32:10.988814 140238231980032 llama.py:574] Using fused attention for tpu
I0310 23:32:11.024286 140249725908992 llama.py:574] Using fused attention for tpu
I0310 23:32:11.407792 140403523438592 llama.py:574] Using fused attention for tpu
I0310 23:32:11.576299 139908633098240 llama.py:574] Using fused attention for tpu
I0310 23:32:12.650510 140590184425472 llama.py:574] Using fused attention for tpu
I0310 23:32:12.885418 140028743354368 llama.py:574] Using fused attention for tpu
I0310 23:32:13.597106 139684034746368 llama.py:574] Using fused attention for tpu
block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768
block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000





Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768
block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000





Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768
block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000





Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768
block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000





Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



block_size 32768


Starting Prefill Testing...
- Context Lengths: 30, Min: 10000, Max: 1000000



  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
I0310 23:32:44.456470 140187042658304 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]I0310 23:32:44.703920 140313205221376 llama.py:574] Using fused attention for tpu
I0310 23:32:44.732551 139682293938176 llama.py:574] Using fused attention for tpu
inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
I0310 23:32:45.071581 139693318477824 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]I0310 23:32:45.438144 139733703407616 llama.py:574] Using fused attention for tpu
inputs.input_ids.shape (1, 32768)
inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
inputs.input_ids.shape (1, 32768)
I0310 23:32:45.604226 140346975668224 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]I0310 23:32:45.754564 140187042658304 llama.py:574] Using fused attention for tpu
inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
I0310 23:32:46.002403 140313205221376 llama.py:574] Using fused attention for tpu
I0310 23:32:46.046584 139682293938176 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]I0310 23:32:46.068145 140396214429696 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
inputs.input_ids.shape (1, 32768)
I0310 23:32:46.281306 139637727176704 llama.py:574] Using fused attention for tpu
I0310 23:32:46.363971 139693318477824 llama.py:574] Using fused attention for tpu
I0310 23:32:46.419755 139761310316544 llama.py:574] Using fused attention for tpu
I0310 23:32:46.484974 139813277104128 llama.py:574] Using fused attention for tpu
I0310 23:32:46.551146 139833263335424 llama.py:574] Using fused attention for tpu
I0310 23:32:46.617815 140063402936320 llama.py:574] Using fused attention for tpu
I0310 23:32:46.618493 139772488329216 llama.py:574] Using fused attention for tpu
I0310 23:32:46.632605 140266063370240 llama.py:574] Using fused attention for tpu
I0310 23:32:46.718091 139733703407616 llama.py:574] Using fused attention for tpu
I0310 23:32:46.844611 140120372094976 llama.py:574] Using fused attention for tpu
I0310 23:32:46.901379 140346975668224 llama.py:574] Using fused attention for tpu
I0310 23:32:46.936595 139789605083136 llama.py:574] Using fused attention for tpu
I0310 23:32:47.011721 140187042658304 llama.py:574] Using fused attention for tpu
I0310 23:32:47.202177 140044392253440 llama.py:574] Using fused attention for tpu
I0310 23:32:47.265026 139968301377536 llama.py:574] Using fused attention for tpu
I0310 23:32:47.265187 140313205221376 llama.py:574] Using fused attention for tpu
I0310 23:32:47.316061 139682293938176 llama.py:574] Using fused attention for tpu
I0310 23:32:47.376307 140396214429696 llama.py:574] Using fused attention for tpu
I0310 23:32:47.568440 139637727176704 llama.py:574] Using fused attention for tpu
I0310 23:32:47.610087 139693318477824 llama.py:574] Using fused attention for tpu
I0310 23:32:47.712431 139761310316544 llama.py:574] Using fused attention for tpu
I0310 23:32:47.774107 139813277104128 llama.py:574] Using fused attention for tpu
I0310 23:32:47.848885 139833263335424 llama.py:574] Using fused attention for tpu
I0310 23:32:47.918386 140063402936320 llama.py:574] Using fused attention for tpu
I0310 23:32:47.929462 139772488329216 llama.py:574] Using fused attention for tpu
I0310 23:32:47.943391 140266063370240 llama.py:574] Using fused attention for tpu
I0310 23:32:47.965241 139733703407616 llama.py:574] Using fused attention for tpu
I0310 23:32:48.128199 140120372094976 llama.py:574] Using fused attention for tpu
I0310 23:32:48.152540 140346975668224 llama.py:574] Using fused attention for tpu
I0310 23:32:48.224507 140187042658304 llama.py:574] Using fused attention for tpu
I0310 23:32:48.238332 139789605083136 llama.py:574] Using fused attention for tpu
I0310 23:32:48.342574 140301649876992 llama.py:574] Using fused attention for tpu
I0310 23:32:48.467988 140313205221376 llama.py:574] Using fused attention for tpu
I0310 23:32:48.477792 140044392253440 llama.py:574] Using fused attention for tpu
I0310 23:32:48.531552 139682293938176 llama.py:574] Using fused attention for tpu
I0310 23:32:48.568137 139968301377536 llama.py:574] Using fused attention for tpu
I0310 23:32:48.636711 140396214429696 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
I0310 23:32:48.807319 139693318477824 llama.py:574] Using fused attention for tpu
I0310 23:32:48.808969 139637727176704 llama.py:574] Using fused attention for tpu
I0310 23:32:48.855447 140312624297984 llama.py:574] Using fused attention for tpu
I0310 23:32:48.980377 139761310316544 llama.py:574] Using fused attention for tpu
I0310 23:32:49.003847 140296425859072 llama.py:574] Using fused attention for tpu
I0310 23:32:49.026591 139813277104128 llama.py:574] Using fused attention for tpu
I0310 23:32:49.120380 139833263335424 llama.py:574] Using fused attention for tpu
I0310 23:32:49.165105 139733703407616 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]I0310 23:32:49.173768 140063402936320 llama.py:574] Using fused attention for tpu
I0310 23:32:49.188001 139772488329216 llama.py:574] Using fused attention for tpu
I0310 23:32:49.210828 140266063370240 llama.py:574] Using fused attention for tpu
inputs.input_ids.shape (1, 32768)
I0310 23:32:49.355208 140346975668224 llama.py:574] Using fused attention for tpu
I0310 23:32:49.382621 140120372094976 llama.py:574] Using fused attention for tpu
I0310 23:32:49.486118 139789605083136 llama.py:574] Using fused attention for tpu
I0310 23:32:49.658477 139986880280576 llama.py:574] Using fused attention for tpu
I0310 23:32:49.707075 140044392253440 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]I0310 23:32:49.819710 140321874843648 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]I0310 23:32:49.832167 139968301377536 llama.py:574] Using fused attention for tpu
I0310 23:32:49.847133 140396214429696 llama.py:574] Using fused attention for tpu
I0310 23:32:49.848034 140109931255808 llama.py:574] Using fused attention for tpu
inputs.input_ids.shape (1, 32768)
inputs.input_ids.shape (1, 32768)
I0310 23:32:50.002681 139637727176704 llama.py:574] Using fused attention for tpu
I0310 23:32:50.192392 139761310316544 llama.py:574] Using fused attention for tpu
I0310 23:32:50.229333 139813277104128 llama.py:574] Using fused attention for tpu
I0310 23:32:50.306353 139829679552512 llama.py:574] Using fused attention for tpu
I0310 23:32:50.330681 139833263335424 llama.py:574] Using fused attention for tpu
I0310 23:32:50.385473 140063402936320 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]I0310 23:32:50.393551 139772488329216 llama.py:574] Using fused attention for tpu
I0310 23:32:50.413154 140266063370240 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
inputs.input_ids.shape (1, 32768)
I0310 23:32:50.582254 140120372094976 llama.py:574] Using fused attention for tpu
I0310 23:32:50.676468 139789605083136 llama.py:574] Using fused attention for tpu
I0310 23:32:50.893301 140044392253440 llama.py:574] Using fused attention for tpu
I0310 23:32:51.037836 139968301377536 llama.py:574] Using fused attention for tpu
I0310 23:32:51.129358 140321874843648 llama.py:574] Using fused attention for tpu
I0310 23:32:51.132437 140109931255808 llama.py:574] Using fused attention for tpu
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
  0%|          | 0/1 [00:00<?, ?it/s]inputs.input_ids.shape (1, 32768)
I0310 23:32:52.153980 140301649876992 llama.py:574] Using fused attention for tpu
I0310 23:32:52.377246 140109931255808 llama.py:574] Using fused attention for tpu
I0310 23:32:52.383977 140321874843648 llama.py:574] Using fused attention for tpu
I0310 23:32:52.677705 140312624297984 llama.py:574] Using fused attention for tpu
I0310 23:32:52.756952 140296425859072 llama.py:574] Using fused attention for tpu
I0310 23:32:53.167172 139684034746368 llama.py:574] Using fused attention for tpu
I0310 23:32:53.410680 139986880280576 llama.py:574] Using fused attention for tpu
I0310 23:32:53.509226 140238231980032 llama.py:574] Using fused attention for tpu
I0310 23:32:53.512399 140249725908992 llama.py:574] Using fused attention for tpu
I0310 23:32:53.576569 140109931255808 llama.py:574] Using fused attention for tpu
I0310 23:32:53.586217 140321874843648 llama.py:574] Using fused attention for tpu
I0310 23:32:54.091132 140403523438592 llama.py:574] Using fused attention for tpu
I0310 23:32:54.097617 139908633098240 llama.py:574] Using fused attention for tpu
I0310 23:32:54.155444 139829679552512 llama.py:574] Using fused attention for tpu
I0310 23:32:54.469235 139684034746368 llama.py:574] Using fused attention for tpu
I0310 23:32:54.810551 140590184425472 llama.py:574] Using fused attention for tpu
I0310 23:32:55.090531 140028743354368 llama.py:574] Using fused attention for tpu
I0310 23:32:55.721708 139684034746368 llama.py:574] Using fused attention for tpu
I0310 23:32:55.924051 140301649876992 llama.py:574] Using fused attention for tpu
I0310 23:32:56.449681 140312624297984 llama.py:574] Using fused attention for tpu
I0310 23:32:56.473344 140296425859072 llama.py:574] Using fused attention for tpu
I0310 23:32:56.926865 139684034746368 llama.py:574] Using fused attention for tpu
I0310 23:32:57.129219 139986880280576 llama.py:574] Using fused attention for tpu
I0310 23:32:57.372538 140249725908992 llama.py:574] Using fused attention for tpu
I0310 23:32:57.384315 140238231980032 llama.py:574] Using fused attention for tpu
I0310 23:32:57.905766 139908633098240 llama.py:574] Using fused attention for tpu
I0310 23:32:57.956915 140403523438592 llama.py:574] Using fused attention for tpu
I0310 23:32:57.971951 139829679552512 llama.py:574] Using fused attention for tpu
I0310 23:32:58.607956 140590184425472 llama.py:574] Using fused attention for tpu
I0310 23:32:58.917835 140028743354368 llama.py:574] Using fused attention for tpu
I0310 23:32:59.564522 140301649876992 llama.py:574] Using fused attention for tpu
I0310 23:33:00.132233 140296425859072 llama.py:574] Using fused attention for tpu
I0310 23:33:00.163560 140312624297984 llama.py:574] Using fused attention for tpu
I0310 23:33:00.791279 139986880280576 llama.py:574] Using fused attention for tpu
I0310 23:33:01.186022 140249725908992 llama.py:574] Using fused attention for tpu
I0310 23:33:01.205517 140238231980032 llama.py:574] Using fused attention for tpu
I0310 23:33:01.632565 139829679552512 llama.py:574] Using fused attention for tpu
I0310 23:33:01.663655 139908633098240 llama.py:574] Using fused attention for tpu
I0310 23:33:01.774241 140403523438592 llama.py:574] Using fused attention for tpu
I0310 23:33:02.355252 140590184425472 llama.py:574] Using fused attention for tpu
I0310 23:33:02.689930 140028743354368 llama.py:574] Using fused attention for tpu
I0310 23:33:04.945593 140249725908992 llama.py:574] Using fused attention for tpu
I0310 23:33:04.975112 140238231980032 llama.py:574] Using fused attention for tpu
I0310 23:33:05.406825 139908633098240 llama.py:574] Using fused attention for tpu
I0310 23:33:05.544369 140403523438592 llama.py:574] Using fused attention for tpu
I0310 23:33:05.997170 140590184425472 llama.py:574] Using fused attention for tpu
I0310 23:33:06.415061 140028743354368 llama.py:574] Using fused attention for tpu
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:02<?, ?it/s]
  0%|          | 0/1 [01:02<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:01<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:02<?, ?it/s]
  0%|          | 0/1 [01:02<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:02<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:01<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:02<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:02<?, ?it/s]
  0%|          | 0/1 [01:02<?, ?it/s]
  0%|          | 0/1 [01:02<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:02<?, ?it/s]
  0%|          | 0/1 [01:02<?, ?it/s]
  0%|          | 0/1 [01:02<?, ?it/s]
  0%|          | 0/1 [01:02<?, ?it/s]
  0%|          | 0/1 [01:02<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:02<?, ?it/s]
  0%|          | 0/1 [01:03<?, ?it/s]
##### Command execution on worker 21 failed with exit status 1. Continuing.
##### Command execution on worker 27 failed with exit status 1. Continuing.
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


##### Command execution on worker 18 failed with exit status 1. Continuing.
  0%|          | 0/1 [01:02<?, ?it/s]
##### Command execution on worker 3 failed with exit status 1. Continuing.
  0%|          | 0/1 [01:02<?, ?it/s]
##### Command execution on worker 12 failed with exit status 1. Continuing.
##### Command execution on worker 19 failed with exit status 1. Continuing.
##### Command execution on worker 0 failed with exit status 1. Continuing.
##### Command execution on worker 30 failed with exit status 1. Continuing.
##### Command execution on worker 8 failed with exit status 1. Continuing.
##### Command execution on worker 28 failed with exit status 1. Continuing.
##### Command execution on worker 6 failed with exit status 1. Continuing.
##### Command execution on worker 1 failed with exit status 1. Continuing.
##### Command execution on worker 23 failed with exit status 1. Continuing.
##### Command execution on worker 13 failed with exit status 1. Continuing.
##### Command execution on worker 17 failed with exit status 1. Continuing.
##### Command execution on worker 10 failed with exit status 1. Continuing.
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


##### Command execution on worker 11 failed with exit status 1. Continuing.
##### Command execution on worker 25 failed with exit status 1. Continuing.
  0%|          | 0/1 [01:02<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:11<?, ?it/s]
##### Command execution on worker 24 failed with exit status 1. Continuing.
##### Command execution on worker 20 failed with exit status 1. Continuing.
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:12<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:13<?, ?it/s]
##### Command execution on worker 29 failed with exit status 1. Continuing.
##### Command execution on worker 31 failed with exit status 1. Continuing.
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


##### Command execution on worker 16 failed with exit status 1. Continuing.
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:17<?, ?it/s]
  0%|          | 0/1 [01:12<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:14<?, ?it/s]
##### Command execution on worker 14 failed with exit status 1. Continuing.
  0%|          | 0/1 [01:13<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:15<?, ?it/s]
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 262, in <module>
    run(main)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/xvjiarui0826/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 259, in main
    ht.start_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 118, in start_test
    self.run_test()
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 100, in run_test
    outs = self.model(contexts_i, max_input_length)
  File "/home/xvjiarui0826/LWM/scripts/eval_speed.py", line 236, in __call__
    output, self.sharded_rng = self._forward_generate(
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 18.05M of 16.00M vmem. Exceeded vmem capacity by 2.05M.

Program vmem requirement 18.05M:
    scoped           18.05M

  Largest program allocations in vmem:

  1. Size: 4.95M
     XLA label: register allocator spill slots call depth 2
     Allocation type: scoped
     ==========================

  2. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  3. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  4. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  5. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  6. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  7. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  8. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  9. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  10. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  11. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  12. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  13. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  14. Size: 1.00M
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[1048576]{0}
     Unpadded size: 1.00M
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  15. Size: 64.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u8[65536]{0}
     Unpadded size: 64.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  16. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  17. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  18. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  19. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  20. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  21. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  22. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================

  23. Size: 4.0K
     Operator: op_name="pjit(fn)/jit(main)/FlaxLLaMAForCausalLMModule/transformer/h/while/body/scan_decoder/attention/jit(shmap_body)/while/body/jit(wrapped)/jit(apply_kernel)/tpu_custom_call[config=CustomCallBackendConfig(<omitted>) kernel_name=func kernel_regeneration_metadata=None out_avals=(ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,1,1024,128]), ShapedArray(float32[1,4,1024,128]), ShapedArray(float32[1,4,1024,128]))]" source_file="/home/xvjiarui0826/LWM/lwm/ring_attention.py" source_line=1160
     Shape: u32[8,128]{1,0}
     Unpadded size: 4.0K
     XLA label: custom-call.45 = custom-call(bitcast.295, bitcast.296, get-tuple-element.2444, copy.357, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[1]{0}, s32[1]{0}, f32[1,4,1024,128]{3,2,1,0}, f32[1,4,2048,128]{3,2,1,0}, f32[1,4,2048,...
     Allocation type: scoped
     ==========================


  0%|          | 0/1 [01:17<?, ?it/s]
  0%|          | 0/1 [01:18<?, ?it/s]
  0%|          | 0/1 [01:18<?, ?it/s]
##### Command execution on worker 15 failed with exit status 1. Continuing.
##### Command execution on worker 4 failed with exit status 1. Continuing.
##### Command execution on worker 22 failed with exit status 1. Continuing.
##### Command execution on worker 5 failed with exit status 1. Continuing.
##### Command execution on worker 9 failed with exit status 1. Continuing.
##### Command execution on worker 26 failed with exit status 1. Continuing.
##### Command execution on worker 2 failed with exit status 1. Continuing.
##### Command execution on worker 7 failed with exit status 1. Continuing.
